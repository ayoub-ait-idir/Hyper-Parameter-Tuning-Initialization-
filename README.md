# Hyper-Parameter-Tuning-
This Notebook illustrates how hyperparameter tuning, Regularization and Optimisation can help improving the performance of our model.

* The first notebook is about the difference between different approches of parameter initialization used in Deep Neural Networks. Initialization with zeros, random initialization and what's called ''He initialization'', and shows the performance of each initialization.

* The second one is about Regularization, Firstly I implemented L2-Regularization that helped overcome overfitting. And tested the impact of Dropout on the performance of our model.
* In the third one, Grandient-Checking is implemented to verify the accuracy of the backpropagation we've implemented.
